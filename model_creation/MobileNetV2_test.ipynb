{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2, l1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"proccesed_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to your dataset (replace with actual path if needed)\n",
    "def load_and_split_data(directory, validation_split=0.2, test_split=0.1, seed=123):\n",
    "    # Load the entire dataset\n",
    "    full_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory,\n",
    "        seed=seed,\n",
    "        shuffle=True,\n",
    "        label_mode='categorical',\n",
    "        image_size=(224, 224),\n",
    "        batch_size=32) \n",
    "\n",
    "    # Calculate the number of batches needed for each split\n",
    "    total_batches = len(full_dataset)\n",
    "    val_batches = int(total_batches * validation_split)\n",
    "    test_batches = int(total_batches * test_split)\n",
    "    train_batches = total_batches - val_batches - test_batches\n",
    "\n",
    "    # Split the dataset into train, validation, and test\n",
    "    train_dataset = full_dataset.take(train_batches)\n",
    "    test_dataset = full_dataset.skip(train_batches).take(test_batches)\n",
    "    validation_dataset = full_dataset.skip(train_batches + test_batches)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8303 files belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "base_dir = 'proccesed_dataset'\n",
    "train_ds, val_ds, test_ds = load_and_split_data(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape, num_classes):\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        layers.RandomRotation(0.2),\n",
    "        layers.RandomZoom(0.2),\n",
    "        layers.RandomTranslation(height_factor=0.2, width_factor=0.2),\n",
    "        layers.RandomContrast(0.2),\n",
    "        layers.RandomBrightness(0.2)\n",
    "    ])\n",
    "    \n",
    "    base_model = MobileNetV2(input_shape=input_shape,\n",
    "                            include_top=False,\n",
    "                            weights='imagenet')\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:-20]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = data_augmentation(inputs)\n",
    "    x = base_model(inputs, training=True)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Increased L2 regularization for weight decay effect\n",
    "    x = Dense(2048, activation='relu', \n",
    "              kernel_regularizer=l2(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.7)(x)\n",
    "    \n",
    "    x = Dense(1024, activation='relu', \n",
    "              kernel_regularizer=l2(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.6)(x)\n",
    "    \n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.0001,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(input_shape=(224, 224, 3), num_classes=6)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=5e-5\n",
    "    ),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3)\n",
      "(32, 6)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_ds.take(1):\n",
    "    print(images.shape)  # Should be (batch_size, 224, 224, 3)\n",
    "    print(labels.shape)  # Should be (batch_size, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccuracyThreshold(Callback):\n",
    "    def __init__(self, threshold=0.85):\n",
    "        super(AccuracyThreshold, self).__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_accuracy = logs.get(\"val_accuracy\")\n",
    "        accuracy = logs.get(\"accuracy\")\n",
    "        if val_accuracy is not None:\n",
    "            if val_accuracy >= self.threshold:\n",
    "                print(f\"\\nReached {self.threshold * 100}% val accuracy. Stopping training...\")\n",
    "                self.model.stop_training = True\n",
    "            elif accuracy >= 0.95:\n",
    "                print(f\"\\nReached 95% accuracy. Stopping training...\")\n",
    "                self.model.stop_training = True\n",
    "\n",
    "# Instantiate the custom callback with a threshold of 85% (0.85)\n",
    "accuracy_threshold_callback = AccuracyThreshold(threshold=0.85)\n",
    "\n",
    "# Early Stopping and Model Checkpoint\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=15,  # More patience\n",
    "    restore_best_weights=True,\n",
    "    min_delta=0.001  # Minimum change to qualify as an improvement\n",
    ")\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'best_model.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "# Learning Rate Reducer\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,  # Reduce learning rate by half\n",
    "    patience=5,\n",
    "    min_lr=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "182/182 [==============================] - 55s 283ms/step - loss: 31.4689 - accuracy: 0.3350 - val_loss: 29.9277 - val_accuracy: 0.4918 - lr: 5.0000e-05\n",
      "Epoch 2/50\n",
      "182/182 [==============================] - 48s 264ms/step - loss: 30.0395 - accuracy: 0.4722 - val_loss: 28.9144 - val_accuracy: 0.5883 - lr: 5.0000e-05\n",
      "Epoch 3/50\n",
      "182/182 [==============================] - 49s 269ms/step - loss: 28.9767 - accuracy: 0.5319 - val_loss: 28.0117 - val_accuracy: 0.6351 - lr: 5.0000e-05\n",
      "Epoch 4/50\n",
      "182/182 [==============================] - 48s 264ms/step - loss: 27.9596 - accuracy: 0.5707 - val_loss: 27.1412 - val_accuracy: 0.6521 - lr: 5.0000e-05\n",
      "Epoch 5/50\n",
      "182/182 [==============================] - 51s 281ms/step - loss: 26.9020 - accuracy: 0.6198 - val_loss: 26.2187 - val_accuracy: 0.6563 - lr: 5.0000e-05\n",
      "Epoch 6/50\n",
      "182/182 [==============================] - 48s 264ms/step - loss: 25.9560 - accuracy: 0.6386 - val_loss: 25.3404 - val_accuracy: 0.6618 - lr: 5.0000e-05\n",
      "Epoch 7/50\n",
      "182/182 [==============================] - 50s 276ms/step - loss: 24.9712 - accuracy: 0.6641 - val_loss: 24.4283 - val_accuracy: 0.6600 - lr: 5.0000e-05\n",
      "Epoch 8/50\n",
      "182/182 [==============================] - 48s 261ms/step - loss: 24.0054 - accuracy: 0.6901 - val_loss: 23.5469 - val_accuracy: 0.6861 - lr: 5.0000e-05\n",
      "Epoch 9/50\n",
      "182/182 [==============================] - 50s 275ms/step - loss: 23.0646 - accuracy: 0.7129 - val_loss: 22.6743 - val_accuracy: 0.6825 - lr: 5.0000e-05\n",
      "Epoch 10/50\n",
      "182/182 [==============================] - 50s 277ms/step - loss: 22.1073 - accuracy: 0.7361 - val_loss: 21.7863 - val_accuracy: 0.6970 - lr: 5.0000e-05\n",
      "Epoch 11/50\n",
      "182/182 [==============================] - 50s 274ms/step - loss: 21.1542 - accuracy: 0.7624 - val_loss: 20.9144 - val_accuracy: 0.6855 - lr: 5.0000e-05\n",
      "Epoch 12/50\n",
      "182/182 [==============================] - 50s 276ms/step - loss: 20.2409 - accuracy: 0.7830 - val_loss: 20.0632 - val_accuracy: 0.7122 - lr: 5.0000e-05\n",
      "Epoch 13/50\n",
      "182/182 [==============================] - 48s 264ms/step - loss: 19.3388 - accuracy: 0.8043 - val_loss: 19.2631 - val_accuracy: 0.7001 - lr: 5.0000e-05\n",
      "Epoch 14/50\n",
      "182/182 [==============================] - 50s 275ms/step - loss: 18.4176 - accuracy: 0.8309 - val_loss: 18.4667 - val_accuracy: 0.6982 - lr: 5.0000e-05\n",
      "Epoch 15/50\n",
      "182/182 [==============================] - 50s 276ms/step - loss: 17.5470 - accuracy: 0.8487 - val_loss: 17.6619 - val_accuracy: 0.7116 - lr: 5.0000e-05\n",
      "Epoch 16/50\n",
      "182/182 [==============================] - 48s 262ms/step - loss: 16.6741 - accuracy: 0.8628 - val_loss: 16.8835 - val_accuracy: 0.7116 - lr: 5.0000e-05\n",
      "Epoch 17/50\n",
      "182/182 [==============================] - 48s 266ms/step - loss: 15.8329 - accuracy: 0.8886 - val_loss: 16.0769 - val_accuracy: 0.7158 - lr: 5.0000e-05\n",
      "Epoch 18/50\n",
      "182/182 [==============================] - 50s 276ms/step - loss: 15.0361 - accuracy: 0.8942 - val_loss: 15.3838 - val_accuracy: 0.7140 - lr: 5.0000e-05\n",
      "Epoch 19/50\n",
      "182/182 [==============================] - 50s 276ms/step - loss: 14.1989 - accuracy: 0.9212 - val_loss: 14.7046 - val_accuracy: 0.7073 - lr: 5.0000e-05\n",
      "Epoch 20/50\n",
      "182/182 [==============================] - 50s 273ms/step - loss: 13.4375 - accuracy: 0.9286 - val_loss: 14.0160 - val_accuracy: 0.7158 - lr: 5.0000e-05\n",
      "Epoch 21/50\n",
      "182/182 [==============================] - 48s 261ms/step - loss: 12.6825 - accuracy: 0.9361 - val_loss: 13.3259 - val_accuracy: 0.7134 - lr: 5.0000e-05\n",
      "Epoch 22/50\n",
      "182/182 [==============================] - 50s 273ms/step - loss: 11.9411 - accuracy: 0.9452 - val_loss: 12.6637 - val_accuracy: 0.7073 - lr: 5.0000e-05\n",
      "Epoch 23/50\n",
      "182/182 [==============================] - 50s 274ms/step - loss: 11.2311 - accuracy: 0.9485 - val_loss: 12.0031 - val_accuracy: 0.7013 - lr: 5.0000e-05\n",
      "Epoch 24/50\n",
      "182/182 [==============================] - ETA: 0s - loss: 10.5375 - accuracy: 0.9581\n",
      "Reached 95% accuracy. Stopping training...\n",
      "182/182 [==============================] - 47s 260ms/step - loss: 10.5375 - accuracy: 0.9581 - val_loss: 11.3933 - val_accuracy: 0.7171 - lr: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=50,\n",
    "    callbacks=[accuracy_threshold_callback,early_stopping,model_checkpoint,reduce_lr] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 5s 164ms/step - loss: 11.1928 - accuracy: 0.7656\n",
      "Test accuracy: 0.765625\n",
      "Test loss: 11.19279670715332\n"
     ]
    }
   ],
   "source": [
    "# Assuming test_ds is your test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "print(\"Test accuracy:\", test_accuracy)\n",
    "print(\"Test loss:\", test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model//MobileNetModelV2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model//MobileNetModelV2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('model//MobileNetModelV2')  # Saves to HDF5 file (requires h5py installed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "# Save to CSV\n",
    "history_df.to_csv('model_history2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(dataset):\n",
    "    labels_list = []\n",
    "    for _, labels in dataset:\n",
    "        labels_list.append(labels.numpy())\n",
    "    return np.concatenate(labels_list, axis=0)\n",
    "\n",
    "val_labels = extract_labels(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 10s 160ms/step\n",
      "[[48 34 40 31 50 40]\n",
      " [46 55 56 37 58 64]\n",
      " [46 49 60 33 56 52]\n",
      " [40 25 41 25 28 36]\n",
      " [46 50 67 43 57 52]\n",
      " [35 53 50 27 63 54]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.20      0.19       243\n",
      "           1       0.21      0.17      0.19       316\n",
      "           2       0.19      0.20      0.20       296\n",
      "           3       0.13      0.13      0.13       195\n",
      "           4       0.18      0.18      0.18       315\n",
      "           5       0.18      0.19      0.19       282\n",
      "\n",
      "    accuracy                           0.18      1647\n",
      "   macro avg       0.18      0.18      0.18      1647\n",
      "weighted avg       0.18      0.18      0.18      1647\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# After training\n",
    "y_pred = model.predict(val_ds)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(val_labels, axis=1)\n",
    "\n",
    "print(confusion_matrix(y_true_classes, y_pred_classes))\n",
    "print(classification_report(y_true_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trash_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
