{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adria\\.conda\\envs\\trash_app\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "from tensorflow.keras.applications import EfficientNetB7\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2, l1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"proccesed_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to your dataset (replace with actual path if needed)\n",
    "def load_and_split_data(directory, validation_split=0.2, test_split=0.1, seed=123):\n",
    "    # Load the entire dataset\n",
    "    full_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory,\n",
    "        seed=seed,\n",
    "        shuffle=True,\n",
    "        label_mode='categorical',\n",
    "        image_size=(224, 224),\n",
    "        batch_size=32) \n",
    "\n",
    "    # Calculate the number of batches needed for each split\n",
    "    total_batches = len(full_dataset)\n",
    "    val_batches = int(total_batches * validation_split)\n",
    "    test_batches = int(total_batches * test_split)\n",
    "    train_batches = total_batches - val_batches - test_batches\n",
    "\n",
    "    # Split the dataset into train, validation, and test\n",
    "    train_dataset = full_dataset.take(train_batches)\n",
    "    test_dataset = full_dataset.skip(train_batches).take(test_batches)\n",
    "    validation_dataset = full_dataset.skip(train_batches + test_batches)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8303 files belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "base_dir = 'proccesed_dataset'\n",
    "train_ds, val_ds, test_ds = load_and_split_data(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape, num_classes):\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        layers.RandomRotation(0.2),\n",
    "        layers.RandomZoom(0.2),\n",
    "        layers.RandomTranslation(height_factor=0.2, width_factor=0.2),\n",
    "        layers.RandomContrast(0.2),\n",
    "        layers.RandomBrightness(0.2)\n",
    "    ])\n",
    "    \n",
    "    base_model = EfficientNetB7(input_shape=input_shape,\n",
    "                            include_top=False,\n",
    "                            weights='imagenet')\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:-20]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = data_augmentation(inputs)\n",
    "    x = base_model(inputs, training=True)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Increased L2 regularization for weight decay effect\n",
    "    x = Dense(2048, activation='relu', \n",
    "              kernel_regularizer=l2(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.7)(x)\n",
    "    \n",
    "    x = Dense(1024, activation='relu', \n",
    "              kernel_regularizer=l2(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.6)(x)\n",
    "    \n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(input_shape=(224, 224, 3), num_classes=6)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "    learning_rate=6e-5\n",
    "    ),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccuracyThreshold(Callback):\n",
    "    def __init__(self, threshold=0.85):\n",
    "        super(AccuracyThreshold, self).__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_accuracy = logs.get(\"val_accuracy\")\n",
    "        accuracy = logs.get(\"accuracy\")\n",
    "        if val_accuracy is not None:\n",
    "            if val_accuracy >= self.threshold:\n",
    "                print(f\"\\nReached {self.threshold * 100}% val accuracy. Stopping training...\")\n",
    "                self.model.stop_training = True\n",
    "            elif accuracy >= 0.95:\n",
    "                print(f\"\\nReached 95% accuracy. Stopping training...\")\n",
    "                self.model.stop_training = True\n",
    "\n",
    "# Instantiate the custom callback with a threshold of 85% (0.85)\n",
    "accuracy_threshold_callback = AccuracyThreshold(threshold=0.90)\n",
    "\n",
    "# Early Stopping and Model Checkpoint\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=5,  # More patience\n",
    "    restore_best_weights=True,\n",
    "    min_delta=0.001  # Minimum change to qualify as an improvement\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "182/182 [==============================] - 552s 3s/step - loss: 37.1595 - accuracy: 0.5737 - val_loss: 35.5565 - val_accuracy: 0.8221\n",
      "Epoch 2/50\n",
      "182/182 [==============================] - 534s 3s/step - loss: 34.9331 - accuracy: 0.7641 - val_loss: 33.7193 - val_accuracy: 0.8597\n",
      "Epoch 3/50\n",
      "182/182 [==============================] - 528s 3s/step - loss: 33.0659 - accuracy: 0.8110 - val_loss: 31.9319 - val_accuracy: 0.8767\n",
      "Epoch 4/50\n",
      "182/182 [==============================] - 529s 3s/step - loss: 31.1927 - accuracy: 0.8388 - val_loss: 30.1299 - val_accuracy: 0.8871\n",
      "Epoch 5/50\n",
      "182/182 [==============================] - 516s 3s/step - loss: 29.3278 - accuracy: 0.8599 - val_loss: 28.3268 - val_accuracy: 0.8919\n",
      "Epoch 6/50\n",
      "182/182 [==============================] - 526s 3s/step - loss: 27.4895 - accuracy: 0.8743 - val_loss: 26.5533 - val_accuracy: 0.8962\n",
      "Epoch 7/50\n",
      "182/182 [==============================] - ETA: 0s - loss: 25.6696 - accuracy: 0.8934\n",
      "Reached 90.0% val accuracy. Stopping training...\n",
      "182/182 [==============================] - 515s 3s/step - loss: 25.6696 - accuracy: 0.8934 - val_loss: 24.8021 - val_accuracy: 0.9035\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=50,\n",
    "    callbacks=[accuracy_threshold_callback,early_stopping] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 56s 2s/step - loss: 24.7946 - accuracy: 0.9171\n",
      "Test accuracy: 0.917067289352417\n",
      "Test loss: 24.794639587402344\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "print(\"Test accuracy:\", test_accuracy)\n",
    "print(\"Test loss:\", test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    for attr, value in layer.__dict__.items():\n",
    "        if isinstance(value, tf.Tensor):\n",
    "            print(f\"Layer {layer.name} has a non-serializable attribute {attr} with value {value}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 273). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/EfficientNetB7\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/EfficientNetB7\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('model/EfficientNetB7')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "# Save to CSV\n",
    "history_df.to_csv('model_history.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trash_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
